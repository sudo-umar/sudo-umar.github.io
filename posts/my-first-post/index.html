<!doctype html><html lang=en-us><head><link rel=preload href=/lib/font-awesome/webfonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/font-awesome/webfonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/font-awesome/webfonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><script type=text/javascript src=https://latest.cactus.chat/cactus.js></script>
<link rel=stylesheet href=https://latest.cactus.chat/style.css type=text/css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Simple Neural Network in Pytorch | Umar Akram</title><link rel=canonical href=https://sudo-umar.github.io/posts/my-first-post/><meta name=description content="This mini-library contains some fun stuff and involved topics explained in the easiest way possible. I tried to add some vanilla flavor in the boring topics, so they could be less boring to read ."><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="Simple Neural Network in Pytorch"><meta property="og:description" content="The only pre-requeste to understanding this simple article is just the basic understanding of python.
We are going to create a simple regressor - a model which is used for predicting the continuous variable - in pytorch.
Dataset First, create a dataset containing our observations(examples) and the labels associated with each observation.
import torch import numpy # column 1 mu1,sig1 = 0,0.1 x1 = np.random.normal(mu1,sig1,(100)) x1 = x1.reshape(100,1) # column 2 mu2,sig2 = 1,1."><meta property="og:type" content="article"><meta property="og:url" content="https://sudo-umar.github.io/posts/my-first-post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-07T15:26:15+02:00"><meta property="article:modified_time" content="2022-06-07T15:26:15+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Simple Neural Network in Pytorch"><meta name=twitter:description content="The only pre-requeste to understanding this simple article is just the basic understanding of python.
We are going to create a simple regressor - a model which is used for predicting the continuous variable - in pytorch.
Dataset First, create a dataset containing our observations(examples) and the labels associated with each observation.
import torch import numpy # column 1 mu1,sig1 = 0,0.1 x1 = np.random.normal(mu1,sig1,(100)) x1 = x1.reshape(100,1) # column 2 mu2,sig2 = 1,1."><link rel=stylesheet href=https://sudo-umar.github.io/css/styles.94f653e9e151e28067a7c5dbbc4600cbd5a3c721e79faaf971e523c40f3b249b8e4f20bb57810dfffa8d559ca5c140fd56eb4cd9c0853113ad08e66afdb08bdd.css integrity="sha512-lPZT6eFR4oBnp8XbvEYAy9WjxyHnn6r5ceUjxA87JJuOTyC7V4EN//qNVZylwUD9VutM2cCFMROtCOZq/bCL3Q=="><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://sudo-umar.github.io/images/favicon.ico></head><body class="max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick='$("html, body").animate({scrollTop:0},"fast")' style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=/posts>All posts</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://sudo-umar.github.io/posts/python_tips/ aria-label=Previous><i class="fas fa-chevron-left" aria-hidden=true onmouseover='$("#i-prev").toggle()' onmouseout='$("#i-prev").toggle()'></i></a></li><li><a class=icon href=https://sudo-umar.github.io/about/aboutme/ aria-label=Next><i class="fas fa-chevron-right" aria-hidden=true onmouseover='$("#i-next").toggle()' onmouseout='$("#i-next").toggle()'></i></a></li><li><a class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up" aria-hidden=true onmouseover='$("#i-top").toggle()' onmouseout='$("#i-top").toggle()'></i></a></li><li><a class=icon href=# aria-label=Share><i class="fas fa-share-alt" aria-hidden=true onmouseover='$("#i-share").toggle()' onmouseout='$("#i-share").toggle()' onclick='return $("#share").toggle(),!1'></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f" aria-label=Facebook><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&text=Simple%20Neural%20Network%20in%20Pytorch" aria-label=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&title=Simple%20Neural%20Network%20in%20Pytorch" aria-label=Linkedin><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&is_video=false&description=Simple%20Neural%20Network%20in%20Pytorch" aria-label=Pinterest><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=Simple%20Neural%20Network%20in%20Pytorch&body=Check out this article: https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f" aria-label=Email><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&title=Simple%20Neural%20Network%20in%20Pytorch" aria-label=Pocket><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&title=Simple%20Neural%20Network%20in%20Pytorch" aria-label=reddit><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&name=Simple%20Neural%20Network%20in%20Pytorch&description=The%20only%20pre-requeste%20to%20understanding%20this%20simple%20article%20is%20just%20the%20basic%20understanding%20of%20python.%0aWe%20are%20going%20to%20create%20a%20simple%20regressor%20-%20a%20model%20which%20is%20used%20for%20predicting%20the%20continuous%20variable%20-%20in%20pytorch.%0aDataset%20First%2c%20create%20a%20dataset%20containing%20our%20observations%28examples%29%20and%20the%20labels%20associated%20with%20each%20observation.%0aimport%20torch%20import%20numpy%20%23%20column%201%20mu1%2csig1%20%3d%200%2c0.1%20x1%20%3d%20np.random.normal%28mu1%2csig1%2c%28100%29%29%20x1%20%3d%20x1.reshape%28100%2c1%29%20%23%20column%202%20mu2%2csig2%20%3d%201%2c1." aria-label=Tumblr><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&t=Simple%20Neural%20Network%20in%20Pytorch" aria-label="Hacker News"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><nav id=TableOfContents><ul><li><a href=#dataset>Dataset</a></li><li><a href=#convert-input-data-to-tensors>Convert input data to tensors</a></li><li><a href=#define-neural-network>Define Neural Network</a></li><li><a href=#train-our-nn>Train our NN</a></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">Simple Neural Network in Pytorch</h1><div class=meta><div class=postdate><time datetime="2022-06-07 15:26:15 +0200 +0200" itemprop=datePublished>2022-06-07</time></div><div class=article-tag><i class="fas fa-tag"></i>
<a class=tag-link href=/tags/pytorch rel=tag>Pytorch</a></div></div></header><div class=content itemprop=articleBody><p>The only pre-requeste to understanding this simple article is just the basic understanding of python.</p><p>We are going to create a simple regressor - a model which is used for predicting the continuous variable - in pytorch.</p><h2 id=dataset>Dataset</h2><p>First, create a dataset containing our observations(examples) and the labels associated with each observation.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy
</span></span><span style=display:flex><span><span style=color:#75715e># column 1</span>
</span></span><span style=display:flex><span>mu1,sig1 <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>x1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(mu1,sig1,(<span style=color:#ae81ff>100</span>))
</span></span><span style=display:flex><span>x1 <span style=color:#f92672>=</span> x1<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># column 2</span>
</span></span><span style=display:flex><span>mu2,sig2 <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1.2</span>
</span></span><span style=display:flex><span>x2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(mu2,sig2,(<span style=color:#ae81ff>100</span>))
</span></span><span style=display:flex><span>x2 <span style=color:#f92672>=</span> x2<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># concatenate both columns</span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate((x1,x2),axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#labels</span>
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sin(x1)<span style=color:#f92672>+</span>np<span style=color:#f92672>.</span>power(x2,<span style=color:#ae81ff>2</span>)
</span></span></code></pre></div><p>In the first line, we imported torch. Consider it as a toolbox which contains all the necessary tools to build our neural network.</p><p>Next, our observations are represented by a matrix <strong>x</strong> which is of size (100,2) - 100 observations and 2 values per observation(also called atrribute/feature/column). Each individual vector x1 and x2 are generated from a normal distrubtion. The label variable is <strong>y</strong> which is of size (100,1). One label for each observation.</p><h2 id=convert-input-data-to-tensors>Convert input data to tensors</h2><p>The next step is to convert these numpy arrays to tensors. Tensors are just like arrays but they are for pytorch.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>xt <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Tensor(x)
</span></span><span style=display:flex><span>yt <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Tensor(y)
</span></span></code></pre></div><h2 id=define-neural-network>Define Neural Network</h2><p>After that, now we should define our neural network.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>naiveRegressor</span>(torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>    super(naiveRegressor,self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>layer1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>act1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>ReLU()
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>layer2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>20</span>,<span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>act2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>ReLU()
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>out <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>20</span>,<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>xavier_uniform_(self<span style=color:#f92672>.</span>layer1<span style=color:#f92672>.</span>weight)
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>zeros_(self<span style=color:#f92672>.</span>layer1<span style=color:#f92672>.</span>bias)
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>xavier_uniform_(self<span style=color:#f92672>.</span>layer2<span style=color:#f92672>.</span>weight)
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>zeros_(self<span style=color:#f92672>.</span>layer2<span style=color:#f92672>.</span>bias)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self,x):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer1(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>act1(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer2(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span>  self<span style=color:#f92672>.</span>act2(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>out(x)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> naiveRegressor()
</span></span><span style=display:flex><span>mse_loss <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>MSELoss()
</span></span><span style=display:flex><span>optim <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(),lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>)
</span></span></code></pre></div><p>Our NN is actually a class and torch.nn.Module is a base class for all neural network modules. In init function, we define all of our NN layers, activation functions and kind of initialization for our weights and biases. Weights and biases are the parameters that our NN learn during the training phase. Lets go line by line. First we define layer1 of size (2,20). The first number should be the number of features which is 2 in our case as our x size is [100,2]. The number 20 could be anything it is the number of hidden neurons. In the next line, we applied activation function whose job is to introduce non-linearity for better generalization. Same goes for the layer 2 and activation 2. The last layer is output whose size is (20,1). 1 means that the final output is a single value.</p><p>Next we initialize weights and biases. If <strong>i</strong> is the number of input nodes/hidden nodes and <strong>j</strong> is the number of hidden nodes/output nodes, then the number of weights will be <strong>ixj</strong>.You can extend this to multiple layers and add them up for total number of weights. In our case it is, (2x20)+(20x20)+(20+1) = 320,000. One bias term is attached to each hidden and output node so biases will be (20+20+1) = 41. You don&rsquo;t need to explicitly initialize them as their is a default initialization. But, it is a good practice to self initialize as you would be hands on when you needed to initialize with different form of initialization like uniform initialization etc.</p><p>In the forward function, we simply passing our input from one layer to the next and finally through the output layer(self.out).</p><p>In the next three lines after the class, we define our model object, loss function(function to calculate the deviation of our prediction from actual labels/target variable) and optimizer(method for optimizing our objective function).</p><p>Now everything is set-up. We should train our model now.</p><h2 id=train-our-nn>Train our NN</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>n_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>500</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n_epochs):
</span></span><span style=display:flex><span>  <span style=color:#75715e>#forward pass</span>
</span></span><span style=display:flex><span>  l <span style=color:#f92672>=</span> model(xt)
</span></span><span style=display:flex><span>  <span style=color:#75715e>#loss </span>
</span></span><span style=display:flex><span>  J <span style=color:#f92672>=</span> mse_loss(yt,l)
</span></span><span style=display:flex><span>  <span style=color:#75715e># clear the gradients</span>
</span></span><span style=display:flex><span>  optim<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>  <span style=color:#75715e>#compute grad</span>
</span></span><span style=display:flex><span>  J<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>  <span style=color:#75715e># step</span>
</span></span><span style=display:flex><span>  optim<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>  print(<span style=color:#e6db74>&#34;cuurent epoch </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> and error value</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(i,J<span style=color:#f92672>.</span>item()))
</span></span></code></pre></div><p>Start our loop our n_epochs. It means we should pass our whole dataset through our model 500 times. In each iteration, we try to minimize the gap between the predicted value and actual value. The training part consists of 5 steps:</p><ol><li>Forward Pass: Predict outputs(l) by passing our dataset into out NN model.</li><li>Calculate loss: calculate the loss which is the distance between our predicted value comes from the forward pass and the actual target variable value(y).</li><li>Clear the gradients.</li><li>Compute gradients(J.backward())</li><li>Update parameters(weights and biases): through optim.step() function.</li></ol><p>We will see the output such as given below. In which we can see that our error is reducing with each epoch.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>cuurent epoch <span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> error value13<span style=color:#ae81ff>.367425918579102</span>
</span></span><span style=display:flex><span>cuurent epoch <span style=color:#ae81ff>1</span> <span style=color:#f92672>and</span> error value13<span style=color:#ae81ff>.296558380126953</span>
</span></span><span style=display:flex><span>cuurent epoch <span style=color:#ae81ff>2</span> <span style=color:#f92672>and</span> error value13<span style=color:#ae81ff>.22631549835205</span>
</span></span><span style=display:flex><span>cuurent epoch <span style=color:#ae81ff>3</span> <span style=color:#f92672>and</span> error value13<span style=color:#ae81ff>.156633377075195</span>
</span></span><span style=display:flex><span>cuurent epoch <span style=color:#ae81ff>4</span> <span style=color:#f92672>and</span> error value13<span style=color:#ae81ff>.087475776672363</span>
</span></span><span style=display:flex><span>cuurent epoch <span style=color:#ae81ff>5</span> <span style=color:#f92672>and</span> error value13<span style=color:#ae81ff>.018775939941406</span>
</span></span><span style=display:flex><span>cuurent epoch <span style=color:#ae81ff>6</span> <span style=color:#f92672>and</span> error value12<span style=color:#ae81ff>.9505033493042</span>
</span></span><span style=display:flex><span>cuurent epoch <span style=color:#ae81ff>7</span> <span style=color:#f92672>and</span> error value12<span style=color:#ae81ff>.882621765136719</span>
</span></span><span style=display:flex><span>cuurent epoch <span style=color:#ae81ff>8</span> <span style=color:#f92672>and</span> error value12<span style=color:#ae81ff>.815115928649902</span>
</span></span><span style=display:flex><span>cuurent epoch <span style=color:#ae81ff>9</span> <span style=color:#f92672>and</span> error value12<span style=color:#ae81ff>.747956275939941</span>
</span></span><span style=display:flex><span>cuurent epoch <span style=color:#ae81ff>10</span> <span style=color:#f92672>and</span> error value12<span style=color:#ae81ff>.681158065795898</span>
</span></span></code></pre></div><p>Now we can call our trained model to make predictions for us</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model(xt[<span style=color:#ae81ff>0</span>])<span style=color:#f92672>.</span>detach()
</span></span></code></pre></div><p>This is not going to be quite accurate as this was just a simple NN designed for the better explainability.</p></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=/posts>All posts</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#dataset>Dataset</a></li><li><a href=#convert-input-data-to-tensors>Convert input data to tensors</a></li><li><a href=#define-neural-network>Define Neural Network</a></li><li><a href=#train-our-nn>Train our NN</a></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f" aria-label=Facebook><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&text=Simple%20Neural%20Network%20in%20Pytorch" aria-label=Twitter><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&title=Simple%20Neural%20Network%20in%20Pytorch" aria-label=Linkedin><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&is_video=false&description=Simple%20Neural%20Network%20in%20Pytorch" aria-label=Pinterest><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=Simple%20Neural%20Network%20in%20Pytorch&body=Check out this article: https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f" aria-label=Email><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&title=Simple%20Neural%20Network%20in%20Pytorch" aria-label=Pocket><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&title=Simple%20Neural%20Network%20in%20Pytorch" aria-label=reddit><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&name=Simple%20Neural%20Network%20in%20Pytorch&description=The%20only%20pre-requeste%20to%20understanding%20this%20simple%20article%20is%20just%20the%20basic%20understanding%20of%20python.%0aWe%20are%20going%20to%20create%20a%20simple%20regressor%20-%20a%20model%20which%20is%20used%20for%20predicting%20the%20continuous%20variable%20-%20in%20pytorch.%0aDataset%20First%2c%20create%20a%20dataset%20containing%20our%20observations%28examples%29%20and%20the%20labels%20associated%20with%20each%20observation.%0aimport%20torch%20import%20numpy%20%23%20column%201%20mu1%2csig1%20%3d%200%2c0.1%20x1%20%3d%20np.random.normal%28mu1%2csig1%2c%28100%29%29%20x1%20%3d%20x1.reshape%28100%2c1%29%20%23%20column%202%20mu2%2csig2%20%3d%201%2c1." aria-label=Tumblr><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fsudo-umar.github.io%2fposts%2fmy-first-post%2f&t=Simple%20Neural%20Network%20in%20Pytorch" aria-label="Hacker News"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu-toggle class=icon href=# onclick='return $("#nav-footer").toggle(),!1' aria-label=Menu><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick='return $("#toc-footer").toggle(),!1' aria-label=TOC><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick='return $("#share-footer").toggle(),!1' aria-label=Share><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2022 Umar Akram</div><div class=footer-right><nav><ul><li><a href=/>Home</a></li><li><a href=/posts>All posts</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=/lib/font-awesome/css/all.min.css><script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>
<script src=/js/code-copy.js></script></html>